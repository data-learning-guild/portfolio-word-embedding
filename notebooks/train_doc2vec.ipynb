{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f453e2-1458-48ea-9abd-8a79a9c1bc13",
   "metadata": {},
   "source": [
    "# Slackの会話データのベクタライズモデル（Doc2Vec）を学習させる\n",
    "\n",
    "---\n",
    "\n",
    "## 1. はじめに\n",
    "\n",
    "本ノートブックでは、Slack上の会話データをベクトル化する機械学習モデルの構築を行います。\n",
    "\n",
    "機械学習のアルゴリズムには、Doc2Vecを利用します。\n",
    "\n",
    "**本 Notebook は Google Colab 上で実行することを前提としています。**\n",
    "\n",
    "\n",
    "## 2. 事前に準備していること\n",
    "\n",
    "本ノートブックの作成前に、すでに以下のことが完了していますので、ノートブック内では本処理を実行するコードやその解説は述べません。\n",
    "\n",
    "\n",
    "- Slackの会話情報の抽出と整形\n",
    "- 上記整形済みデータをGoogleDriveにアップロード（`dataset.json`）\n",
    "\n",
    "\n",
    "\n",
    "## 3. 本ノートブックの目的\n",
    "\n",
    "本ノートブックの目的は、以下の３つの処理を実行すること、及びその解説をすることです。\n",
    "\n",
    "- Doc2Vecモデルの学習\n",
    "- 学習済みモデルの動作確認\n",
    "- 学習済みモデルの保存\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7232b6c6-d422-447a-9e8c-0eb6f634f45e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. 処理フロー\n",
    "\n",
    "1. GoogleDriveのマウント\n",
    "2. 必要なパッケージのインストール／インポート\n",
    "3. データセットのロード\n",
    "4. データセットの確認\n",
    "5. Doc2Vecモデルの学習\n",
    "6. 学習済みモデルの動作確認\n",
    "7. 学習済みモデルの保存\n",
    "\n",
    "<br>\n",
    "\n",
    "### 4.1. 処理ごとの目的と入出力一覧\n",
    "\n",
    "|項番|目的|入力|出力|\n",
    "|:-:|:--|:--|:--|\n",
    "|1|GoogleDriveとColabNotebookを接続<br>（データセット読込の準備）|−|−|\n",
    "|2|学習、検証処理などに利用するパッケージを使用可能な状態にする|−|−|\n",
    "|3|データセットをメモリ上で操作可能な状態にする|ファイルパス|データセット|\n",
    "|4|学習後の手戻りを抑制するために、<br>データセットに予期せぬ誤りがないか確認する|データセット|データセットの中身（標準出力）|\n",
    "|5|学習させたいデータセットにモデルを適応させる|未学習モデル|学習済みモデル|\n",
    "|6|学習済みモデルが予期した動作をしているか簡易確認を行い、<br>後工程の手戻りを抑制する|学習済みモデル<br>/検証用デモデータ|計算結果|\n",
    "|7|学習結果を再利用できるようにファイル化する|−|学習済みモデルファイル|\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c95c6e-f0fa-4545-a427-d0e4deb0ee49",
   "metadata": {},
   "source": [
    "## 5. 処理詳細"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65637c0e-3226-4402-9c65-2991676db20c",
   "metadata": {},
   "source": [
    "### 5.1. GoogleDriveのマウント\n",
    "\n",
    "GoogleColabにデフォルトで搭載されている `google.colab` パッケージを使って、自身のGoogleDriveをマウントします。\n",
    "\n",
    "これにより、GoogleDriveをローカルファイルシステムのように扱えるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc7432-f966-4e9a-9fc1-b89ed24d9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5206af4-f149-4a0a-a4e4-cb263e55ea28",
   "metadata": {},
   "source": [
    "### 5.2. 必要なパッケージのインストール／インポート\n",
    "\n",
    "今回のDoc2Vecモデルの学習に必要なパッケージをまとめて、インストール／インポートします。\n",
    "\n",
    "今回利用するパッケージとその用途を下記にまとめます。\n",
    "\n",
    "GoogleColabにデフォルトで組み込まれていないパッケージは、明示的に `pip install ...` を実行します。\n",
    "\n",
    "\n",
    "|分類|パッケージ名|用途|\n",
    "|:-:|---|:--|\n",
    "|標準ライブラリ|json|データセット（JSON形式）の操作の為|\n",
    "||smart_open|大規模データのファイルストリーミング用（通常データセットは大規模である場合が多い）|\n",
    "|3rdpartyライブラリ|numpy|COS類似度算出処理のため|\n",
    "||gensim|Doc2Vecを使用するため|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f1646-be99-4875-bcb0-440187a934a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89d74d-5add-4332-89ee-5e2127f82300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import smart_open\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620fc83-8aba-46df-8d15-9596e210b510",
   "metadata": {},
   "source": [
    "### 5.3. データセットのロード\n",
    "\n",
    "GoogleDriveに格納しているデータセットをロードします。\n",
    "\n",
    "`dataset.json`の構造は以下のようになっています。\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"tag\": 0,\n",
    "    \"text\": [\"word0\", \"word1\", \"word2\"...]\n",
    "  },\n",
    "  {\n",
    "    \"tag\": 1,\n",
    "    \"text\": [\"word0\", \"word1\", \"word2\"...]\n",
    "  },\n",
    "  :\n",
    "]\n",
    "```\n",
    "\n",
    "これらをgensimのTggedDocumentインスタンスに格納しています。\n",
    "\n",
    "\n",
    "【参考】：データセットのTggedDocument形式へのロードは、[gensimの公式ドキュメント](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html?highlight=dataset#define-a-function-to-read-and-preprocess-text)を参照して実装しました。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99267d76-5bef-4175-9dbc-4742eb61be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "        for doc in dataset:\n",
    "          doc_tag = int(doc['tag'])\n",
    "          doc_text = doc['text']\n",
    "          if tokens_only:\n",
    "            yield doc_text\n",
    "          else:\n",
    "            # For training data, add tags\n",
    "            yield TaggedDocument(doc_text, [doc_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cba03-ca34-4846-9840-76072b31608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_path = '/content/drive/My Drive/app/data/portfolio-word-embedding/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f084ce-68cf-473c-a991-7312eb7f6a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(root_dir_path + 'dataset.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc869c-0df4-445f-989d-f9de55e81f55",
   "metadata": {},
   "source": [
    "### 5.4. データセットの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1b66e-6b90-49cf-843b-50c474340ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see coupus\n",
    "train_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895e9c97-5061-488e-80f8-9a1596d7aa27",
   "metadata": {},
   "source": [
    "### 5.5. Doc2Vecモデルの学習\n",
    "\n",
    "ハイパーパラメータを設定して、モデルを学習させます。\n",
    "\n",
    "ハイパーパラメータの設定値、その理由を簡単に述べます。\n",
    "\n",
    "- vector_size\n",
    "  - 算出するベクトルの次元を表します\n",
    "  - 400：[Doc2Vecの論文](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)と同様に設定しました\n",
    "- window\n",
    "  - 一回にいくつの単語をネットワークに読み込むか、という値です\n",
    "  - 8：[Doc2Vecの論文](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)に依れば、多くのアプリケーションでwindow sizeが8~12の範囲が良い結果をもたらした、との説明に基づいて設定しました。\n",
    "- min_count\n",
    "  - 1語と認識するための最低文字数です\n",
    "  - 1：日本語は1文字で1語を表す（「が」など）ことがあるので1に設定しました\n",
    "- epochs\n",
    "  - 1つのデータを何回繰り返し使うかという値です\n",
    "  - 40：いくつかの実装例を参考に設定しました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3585fe14-9e99-46e0-a12c-fe12269eb40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=400, window=8, min_count=1, epochs=40)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c483313-55a2-45e6-aa85-56a885802f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8a96f-280d-423c-aeb4-70cf64af221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727e8ea-f222-4426-bece-b1efb96f76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d922e3-f6ce-4d93-a806-45b5cc752b89",
   "metadata": {},
   "source": [
    "### 5.6. 学習済みモデルの動作確認\n",
    "\n",
    "デモ用データを入力して、文章のベクトル化が期待通りに動いているか、簡単に確認します。\n",
    "\n",
    "文章同士が似ていれば似ているほど、ベクトル同士の類似度が高いことを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b093a72-0425-41a5-9ce1-43bc99bcf74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COS類似度\n",
    "def cos_similarity(_x: list, _y: list) -> float:\n",
    "    vx = np.array(_x)\n",
    "    vy = np.array(_y)\n",
    "    return np.dot(vx, vy) / (np.linalg.norm(vx) * np.linalg.norm(vy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5f6e5d-8315-4ff4-89b5-5a603d638bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_py1_ja4 = model.infer_vector(['python', 'java', 'java', 'java', 'java'])\n",
    "v_py3_ja2 = model.infer_vector(['python', 'python', 'python', 'java', 'java'])\n",
    "v_py4_ja1 = model.infer_vector(['python', 'python', 'python', 'python', 'java'])\n",
    "\n",
    "v_py5_ja0 = model.infer_vector(['python', 'python', 'python', 'python', 'python'])\n",
    "\n",
    "\n",
    "print('「python x 5」文ベクトルとのCOS類似度')\n",
    "print('===============================')\n",
    "print('python x 1, java x 4 >> {}'.format(cos_similarity(v_py5_ja0, v_py1_ja4)))\n",
    "print('python x 3, java x 2 >> {}'.format(cos_similarity(v_py5_ja0, v_py3_ja2)))\n",
    "print('python x 4, java x 1 >> {}'.format(cos_similarity(v_py5_ja0, v_py4_ja1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6bd5cf-f108-4d50-b5eb-945b0fbdc56d",
   "metadata": {},
   "source": [
    "### 5.7. 学習済みモデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba028f-ab18-47a0-89c4-46f25e5ea0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "import pickle\n",
    "with open(root_dir_path + 'trained_doc2vec.model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46334414-a66a-4ca0-af19-6ad7d6dc414f",
   "metadata": {},
   "source": [
    "## 6. 【補足】今後の改善点\n",
    "\n",
    "[Doc2Vecによる文書ベクトル推論の安定化について | Sansan Builders Blog](https://buildersbox.corp-sansan.com/entry/2019/04/10/110000#Doc2Vec%E3%81%AB%E3%82%88%E3%82%8B%E6%96%87%E6%9B%B8%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E6%8E%A8%E8%AB%96%E3%81%AE%E5%95%8F%E9%A1%8C%E7%82%B9)\n",
    "\n",
    "上記参考記事の通り、Doc2Vecには「極めて短い文」のベクトル化を苦手とする側面があります。\n",
    "\n",
    "今回のケースだと、質問文のベクトル化の際、短い文のベクトル化を実行するケースが起こり得る（例：機械学習とは？）ので、対策を打つ必要があると考えています。\n",
    "\n",
    "具体的な対策案としては、今の所以下の２つを考えています。\n",
    "\n",
    "1. Doc2Vec以外のモデルを使う\n",
    "2. 入力文字数に制限（最低入力文字数、単語数）を設定する\n",
    "\n",
    "２番目の対策は、プロダクト開発フェーズで実施すべき項目であることと、本来の課題解決の目的に合致していないことから優先度は低く、１番目の対策を実施すべきと考えています。\n",
    "\n",
    "１番目の対策をより具体的にするならば、会話情報の名詞のみ対象として、Word2Vecによるベクトル化を検討しています。\n",
    "\n",
    "この場合、１ユーザーを高次元の１ベクトルで表現するのではなく、１ユーザーをより低次元の複数のベクトルで表現することになると考えています。\n",
    "\n",
    "\n",
    "![img](https://drive.google.com/uc?export=view&id=13OT9iqbhO54hZlTSD6Z8koWHX12Tsfsw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ccdb2-a8bc-49f3-b5a5-d7badc01728c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
